import os
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
from gensim.models import Word2Vec
import re
import joblib
from collections import Counter
from concurrent.futures import ThreadPoolExecutor
import logging
from sklearn.model_selection import StratifiedKFold

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class WebshellDetector:
    def __init__(self, n_gram_range=(1, 4), feature_size=100, model_type='word2vec'):
        """
        åˆå§‹åŒ–Webshellæ£€æµ‹å™¨
        
        Args:
            n_gram_range: N-gramèŒƒå›´
            feature_size: ç‰¹å¾å‘é‡ç»´åº¦
            model_type: ç‰¹å¾æå–æ¨¡å‹ç±»å‹('tfidf'æˆ–'word2vec')
        """
        self.n_gram_range = n_gram_range
        self.feature_size = feature_size
        self.model_type = model_type
        self.vectorizer = None
        self.scaler = StandardScaler()
        self.model = SVC(probability=True, kernel='rbf', C=10, gamma='scale')
        
    def read_file(self, file_path):
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read()
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
            return ""
    
    def extract_ngrams(self, text, n):
        """æå–æ–‡æœ¬çš„N-gram"""
        text = re.sub(r'\s+', ' ', text)  # æ›¿æ¢è¿ç»­ç©ºæ ¼
        return [text[i:i+n] for i in range(len(text)-n+1)]
    
    def preprocess_file(self, file_path):
        """é¢„å¤„ç†æ–‡ä»¶å†…å®¹ï¼Œæå–ç‰¹å¾"""
        content = self.read_file(file_path)
        if not content:
            return []
        
        # æå–N-gram
        all_ngrams = []
        for n in range(self.n_gram_range[0], self.n_gram_range[1]+1):
            all_ngrams.extend(self.extract_ngrams(content, n))
        
        return all_ngrams
    
    def load_files(self, directory, label):
        """åŠ è½½ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶"""
        logger.info(f"åŠ è½½ç›®å½• {directory} ä¸­çš„æ–‡ä»¶ï¼Œæ ‡ç­¾: {label}")
        file_paths = []
        for root, _, files in os.walk(directory):
            for file in files:
                if file.endswith(('.php', '.jsp')):
                    file_paths.append(os.path.join(root, file))
        
        # å¹¶è¡Œå¤„ç†æ–‡ä»¶
        with ThreadPoolExecutor(max_workers=8) as executor:
            features_list = list(executor.map(self.preprocess_file, file_paths))
        
        # è¿‡æ»¤ç©ºç‰¹å¾
        valid_indices = [i for i, features in enumerate(features_list) if features]
        valid_features = [features_list[i] for i in valid_indices]
        labels = [label] * len(valid_features)
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(valid_features)} ä¸ªæœ‰æ•ˆæ–‡ä»¶")
        return valid_features, labels, [file_paths[i] for i in valid_indices]
    
    def create_tfidf_features(self, features_list):
        """ä½¿ç”¨TF-IDFåˆ›å»ºç‰¹å¾å‘é‡"""
        logger.info("ä½¿ç”¨TF-IDFæå–ç‰¹å¾")
        # å°†ç‰¹å¾åˆ—è¡¨è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
        texts = [' '.join(features) for features in features_list]
        
        # åˆå§‹åŒ–å¹¶æ‹ŸåˆTF-IDFå‘é‡å™¨
        self.vectorizer = TfidfVectorizer(ngram_range=self.n_gram_range, max_features=self.feature_size)
        return self.vectorizer.fit_transform(texts)
    
    def create_word2vec_features(self, features_list):
        """ä½¿ç”¨Word2Vecåˆ›å»ºç‰¹å¾å‘é‡"""
        logger.info("ä½¿ç”¨Word2Vecæå–ç‰¹å¾")
        # è®­ç»ƒWord2Vecæ¨¡å‹
        self.vectorizer = Word2Vec(sentences=features_list, vector_size=self.feature_size, 
                                  window=5, min_count=1, workers=4)
        
        # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºç‰¹å¾å‘é‡
        features = []
        for tokens in features_list:
            if not tokens:
                features.append(np.zeros(self.feature_size))
                continue
                
            # è®¡ç®—æ‰€æœ‰tokençš„å‘é‡å¹³å‡å€¼
            vecs = [self.vectorizer.wv[token] for token in tokens if token in self.vectorizer.wv]
            if not vecs:
                features.append(np.zeros(self.feature_size))
            else:
                features.append(np.mean(vecs, axis=0))
        
        return np.array(features)
    
    def fit(self, features_list, labels, n_splits=5):
        """ä½¿ç”¨äº¤å‰éªŒè¯è®­ç»ƒå¹¶è¯„ä¼°æ¨¡å‹"""
        logger.info(f"å¼€å§‹ä½¿ç”¨{self.model_type}è¿›è¡Œ {n_splits} æŠ˜äº¤å‰éªŒè¯")

        # ç‰¹å¾æå–
        if self.model_type == 'tfidf':
            X = self.create_tfidf_features(features_list).toarray()
        elif self.model_type == 'word2vec':
            X = self.create_word2vec_features(features_list)
        else:
            raise ValueError("model_typeå¿…é¡»æ˜¯'tfidf'æˆ–'word2vec'")

        y = np.array(labels)
        X = self.scaler.fit_transform(X)

        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
        metrics_all = []

        for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):
            logger.info(f"\nâ€”â€” ç¬¬ {fold} æŠ˜ â€”â€”")
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]

            logger.info(f"è®­ç»ƒé›†: {Counter(y_train)}, æµ‹è¯•é›†: {Counter(y_test)}")

            # æ¨¡å‹è®­ç»ƒ
            model = SVC(probability=True, kernel='rbf', C=10, gamma='scale')
            model.fit(X_train, y_train)

            # é¢„æµ‹ä¸è¯„ä¼°
            y_pred = model.predict(X_test)
            metrics = self.evaluate(y_test, y_pred)
            metrics_all.append(metrics)

        # è®¡ç®—å¹³å‡æ€§èƒ½
        avg_metrics = {
            k: np.mean([m[k] for m in metrics_all]) for k in ['accuracy', 'precision', 'recall', 'f1']
        }
        logger.info("\nğŸ“Š å¹³å‡äº¤å‰éªŒè¯ç»“æœ:")
        logger.info(f"å‡†ç¡®ç‡: {avg_metrics['accuracy']:.4f}")
        logger.info(f"ç²¾ç¡®ç‡: {avg_metrics['precision']:.4f}")
        logger.info(f"å¬å›ç‡: {avg_metrics['recall']:.4f}")
        logger.info(f"F1åˆ†æ•°: {avg_metrics['f1']:.4f}")

        # æœ€åè®­ç»ƒå®Œæ•´æ¨¡å‹ç”¨äºé¢„æµ‹
        self.model.fit(X, y)
        return self

    
    def evaluate(self, y_true, y_pred):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, average='binary')
        recall = recall_score(y_true, y_pred, average='binary')
        f1 = f1_score(y_true, y_pred, average='binary')
        
        logger.info("æ¨¡å‹è¯„ä¼°ç»“æœ:")
        logger.info(f"å‡†ç¡®ç‡: {accuracy:.4f}")
        logger.info(f"ç²¾ç¡®ç‡: {precision:.4f}")
        logger.info(f"å¬å›ç‡: {recall:.4f}")
        logger.info(f"F1åˆ†æ•°: {f1:.4f}")
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1
        }
    
    def predict(self, file_path):
        """é¢„æµ‹å•ä¸ªæ–‡ä»¶æ˜¯å¦ä¸ºWebshell"""
        features = self.preprocess_file(file_path)
        if not features:
            return 0  # é»˜è®¤é¢„æµ‹ä¸ºéWebshell
        
        if self.model_type == 'tfidf':
            text = ' '.join(features)
            X = self.vectorizer.transform([text]).toarray()
        elif self.model_type == 'word2vec':
            vecs = [self.vectorizer.wv[token] for token in features if token in self.vectorizer.wv]
            if not vecs:
                X = np.zeros((1, self.feature_size))
            else:
                X = np.mean(vecs, axis=0).reshape(1, -1)
        
        X = self.scaler.transform(X)
        return self.model.predict(X)[0]
    
    def save_model(self, model_path):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'vectorizer': self.vectorizer,
            'scaler': self.scaler,
            'model': self.model,
            'n_gram_range': self.n_gram_range,
            'feature_size': self.feature_size,
            'model_type': self.model_type
        }
        joblib.dump(model_data, model_path)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ° {model_path}")
    
    @staticmethod
    def load_model(model_path):
        """åŠ è½½æ¨¡å‹"""
        model_data = joblib.load(model_path)
        detector = WebshellDetector(
            n_gram_range=model_data['n_gram_range'],
            feature_size=model_data['feature_size'],
            model_type=model_data['model_type']
        )
        detector.vectorizer = model_data['vectorizer']
        detector.scaler = model_data['scaler']
        detector.model = model_data['model']
        return detector

def main():
    # è®¾ç½®æ•°æ®è·¯å¾„
    benign_dir = 'D:/DeepLearning/otherpaper/two/jsp/normal'  # è‰¯æ€§æ–‡ä»¶ç›®å½•
    malicious_dir = 'D:/DeepLearning/otherpaper/two/jsp/shell'  # æ¶æ„æ–‡ä»¶ç›®å½•
    
    # åˆ›å»ºæ£€æµ‹å™¨å®ä¾‹
    detector = WebshellDetector(model_type='word2vec', feature_size=200)
    
    # åŠ è½½æ•°æ®
    benign_features, benign_labels, benign_files = detector.load_files(benign_dir, 0)
    malicious_features, malicious_labels, malicious_files = detector.load_files(malicious_dir, 1)
    
    # åˆå¹¶æ•°æ®é›†
    all_features = benign_features + malicious_features
    all_labels = benign_labels + malicious_labels
    all_files = benign_files + malicious_files
    
    # è®­ç»ƒæ¨¡å‹
    detector.fit(all_features, all_labels)
    

if __name__ == "__main__":
    main()    